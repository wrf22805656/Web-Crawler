import urllib2
def download(url, user_agent='wswp', num_retries=2):
    print "downloading:", url
    headers = {'User-agent': user_agent}
    request = urllib2.Request(url, headers=headers)
    try:
        html = urllib2.urlopen(request).read()
    except urllib2.URLError as e:
        print "download error :" , e.reason
        html = None
    if num_retries > 0:
        if hasattr(e, 'code') and 500 <= e.code < 600:
            # retry 5XX HTTP Errors
            return download(url, user_agent, num_retries-1)
    return html

import itertools

for page in itertools.count(1):
    url = 'http://example.webscraping.com/view/-%d' % page
    html = download(url)
    if html is None:
        break
    else:
        pass
